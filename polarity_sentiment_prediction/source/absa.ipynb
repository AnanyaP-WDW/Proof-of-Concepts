{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "papermill": {
      "duration": 734.839564,
      "end_time": "2020-08-11T07:36:34.949548",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-08-11T07:24:20.109984",
      "version": "2.1.0"
    },
    "colab": {
      "name": "absa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.010811,
          "end_time": "2020-08-11T07:24:25.022604",
          "exception": false,
          "start_time": "2020-08-11T07:24:25.011793",
          "status": "completed"
        },
        "tags": [],
        "id": "y-w9xsAxniGs"
      },
      "source": [
        "## <font size='5' color='red'>Aspect Based Sentiment Analysis</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpOZ7WG_oD80"
      },
      "source": [
        "!pip install git+https://github.com/LIAAD/yake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kTWzG1fbtyf"
      },
      "source": [
        "#### scraper.py\n",
        "\n",
        "\n",
        "# fetch tweets from twitter\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "def get_twitter_data():\n",
        "    \"\"\"Fetch tweets from twitter for ABSA.\"\"\"\n",
        "    # get the data\n",
        "    consumerKey = \"lulAuGwEdpdHK9N2k9rlBffVg\"\n",
        "    consumerSecret = \"qUmfkBnxlbaWZ4HKZRaPpS9fTAicU0fgc8EBszC43yboI3hLZp\"\n",
        "    accessToken = \"1257406807103778818-ZTnOlTd52FzbO9wSyKwJ9Dhv4CBv9b\"\n",
        "    accessTokenSecret = \"llrRS84dOsLY6wQeNYq9TaUvfGgyTi9Jfx9qukLe8J0Mr\"\n",
        "\n",
        "    # authenticate\n",
        "    authenticate = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
        "\n",
        "    # set access token\n",
        "    authenticate.set_access_token(accessToken, accessTokenSecret)\n",
        "\n",
        "    # create the api object\n",
        "    api = tweepy.API(authenticate, wait_on_rate_limit=True)\n",
        "\n",
        "    myCount = 10\n",
        "    search_word = input(\"Enter topic name: \")\n",
        "    public_tweets = api.search(search_word, count=myCount, lang=\"en\")\n",
        "    unwanted_words = ['@', 'RT', ':', 'https', 'http']\n",
        "    symbols = ['@', '#']\n",
        "    single_chars = re.compile(r'\\s+[a-zA-Z]\\s+') # remove single chars\n",
        "    data = []\n",
        "    for tweet in public_tweets:\n",
        "        text = tweet.text\n",
        "        textWords = text.split()\n",
        "        # print(textWords)\n",
        "        cleaning_tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|(RT)\", \" \", text).split())\n",
        "        cleaning_tweet = single_chars.sub('', cleaning_tweet)\n",
        "        data.append(cleaning_tweet)\n",
        "    print(\"===============Tweet Scrapping complete=============\")\n",
        "    data = pd.DataFrame(data)\n",
        "    return data, str(search_word).split()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgJRGBy8bjCo",
        "outputId": "887db84a-d508-441b-f2e0-122f5138f88c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "#### main.py\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import pickle\n",
        "import warnings\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from matplotlib import pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, log_loss, mean_squared_error\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the train data\n",
        "train_data = pd.read_csv('./train.csv')\n",
        "train_data = train_data.groupby('aspect_term').filter(lambda x: len(x) > 10)\n",
        "print(f'data_shape: {train_data.shape}')\n",
        "# shuffle the dataframe\n",
        "train_data = train_data.sample(frac=1, random_state=77).reset_index(drop=True)\n",
        "\n",
        "# Text Cleaning\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;:$!]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_?&]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "single_chars = re.compile(r'\\s+[a-zA-Z]\\s+')\n",
        "\n",
        "def clean_text(text: str)-> str:\n",
        "    \"\"\"\n",
        "    Preprocesses text and returns a cleaned\n",
        "    piece of text with unwanted characters removed\n",
        "    \n",
        "    Args:\n",
        "       text: a string\n",
        "    Returns: \n",
        "        Preprocessed text\n",
        "    \"\"\"\n",
        "    text = str(text).lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwords from text\n",
        "    text = single_chars.sub('', text) #remove single-characters\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_URL(text: str)-> str:\n",
        "    \"\"\"\n",
        "    Removes URL patterns from text.\n",
        "    Args:\n",
        "        `text`: A string, word/sentence\n",
        "    Returns:\n",
        "        Text without url patterns.\n",
        "    \"\"\"\n",
        "    url = re.compile('https?://\\S+|www\\.\\S+')\n",
        "    text = url.sub('',text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def extract_entity(text):\n",
        "    \"\"\"Extract entity name from aspect category.\"\"\"\n",
        "    attribute = re.compile('#[a-zA-Z0-9]+')\n",
        "    underscore = re.compile('_[a-zA-Z0-9]+')\n",
        "    entity = attribute.sub('', str(text).lower())\n",
        "    entity = underscore.sub('', entity)\n",
        "\n",
        "    return entity\n",
        "\n",
        "\n",
        "def count_vectorize(data: str) -> [int]:\n",
        "    \"\"\"\n",
        "    Create word vectors/tokens for input data\n",
        "    Args:\n",
        "        `data`: text to be vectorized\n",
        "    \"\"\"\n",
        "    vectorizer = CountVectorizer(min_df=3, analyzer='word',\n",
        "                                 ngram_range=(1, 3))\n",
        "    vectors = vectorizer.fit_transform(data)\n",
        "\n",
        "    return vectors, vectorizer\n",
        "\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    \"\"\"Computing RMSE metric.\"\"\"\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "\n",
        "def make_dir(dir_name: str):\n",
        "    \"\"\"Creates a new directory in the current working directory.\"\"\"\n",
        "    save_dir = os.path.join('./', dir_name)\n",
        "    if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "    else:\n",
        "        print(f'{save_dir}: Already exists!') \n",
        "    return save_dir\n",
        "\n",
        "\n",
        "# Splitting data into train and test sets\n",
        "train_df = train_data.iloc[:2100]\n",
        "test_df = train_data.iloc[len(train_df):]\n",
        "\n",
        "train_df['text'] = train_df['text'].apply(clean_text)\n",
        "train_df['text'] = train_df['text'].apply(remove_URL)\n",
        "\n",
        "test_df['text'] = test_df['text'].apply(clean_text)\n",
        "test_df['text'] = test_df['text'].apply(remove_URL)\n",
        "\n",
        "train_df['aspect_term'] = train_df['aspect_term'].apply(clean_text)\n",
        "test_df['aspect_term'] = test_df['aspect_term'].apply(clean_text)\n",
        "\n",
        "print(f\"\\nsentiment tweet distribution: {train_df['polarity'].value_counts()}\\n\")\n",
        "\n",
        "#Test-labels\n",
        "polarity_labels_test = pd.get_dummies(test_df['polarity'].astype(str), dtype='int32')\n",
        "aspect_labels_test = pd.get_dummies(test_df['aspect_term'], dtype='int32')\n",
        "\n",
        "# Categorizing polarity and aspect_term labels (multi-label formulation)\n",
        "# Train\n",
        "train_df['polarity'] = train_df['polarity'].astype(str)\n",
        "\n",
        "#remove nan class\n",
        "train_df = train_df[train_df['aspect_term'] != 'nan']\n",
        "test_df = test_df[test_df['aspect_term'] != 'nan']\n",
        "\n",
        "# Class label Encoding\n",
        "aspect_encoder = LabelEncoder()\n",
        "train_df['aspect_term'] = aspect_encoder.fit_transform(train_df['aspect_term'])\n",
        "\n",
        "polarity_encoder = LabelEncoder()\n",
        "train_df['polarity'] = polarity_encoder.fit_transform(train_df['polarity'])\n",
        "\n",
        "\n",
        "aspect_labels_train = pd.get_dummies(train_df['aspect_term'], dtype='int32')\n",
        "\n",
        "\n",
        "print(f\"\\nNumber of unique aspect_terms: {train_df['aspect_term'].nunique()}\\n\")\n",
        "\n",
        "print('\\n========================Text Preprocessing=============================\\n')\n",
        "\n",
        "# Create train vectors\n",
        "train_vectors, count_vectorizer = count_vectorize(train_df['text'])\n",
        "\n",
        "## Map the tokens in the train vectors to the test set.\n",
        "# i.e.the train and test vectors use the same set of tokens\n",
        "test_vectors = count_vectorizer.transform(test_df['text'])\n",
        "\n",
        "# split data into features and labels for training\n",
        "y_aspect = train_df['aspect_term'] #aspect_term target\n",
        "\n",
        "y_polarity = train_df['polarity'] #polarity target\n",
        "\n",
        "# Building the models\n",
        "\n",
        "def build_models():\n",
        "    \"\"\"Define ABSA ensemble predicition models.\n",
        "    Returns:\n",
        "        Absa_model: Aspect prediction model\n",
        "        Polarity_model: Sentiment prediction model\n",
        "    \"\"\"\n",
        "    absa_model = xgb.XGBRegressor(max_depth=5,\n",
        "                              n_estimators=150,\n",
        "                              classes=list(aspect_encoder.classes_),\n",
        "                              colsample_bytree=0.9,\n",
        "                              num_class=79,\n",
        "                              objective='multi:softprob',\n",
        "                              metric='auc',\n",
        "                              nthread=2,\n",
        "                              learning_rate=0.1,\n",
        "                              random_state=77\n",
        "                              )\n",
        "\n",
        "    polarity_model = xgb.XGBRegressor(max_depth=5,\n",
        "                              n_estimators=350,\n",
        "                              classes=list(polarity_encoder.classes_),\n",
        "                              colsample_bytree=0.9,\n",
        "                              num_class=3,\n",
        "                              objective='multi:softprob',\n",
        "                              metric='auc',\n",
        "                              nthread=2,\n",
        "                              learning_rate=0.1,\n",
        "                              random_state=77\n",
        "                              )\n",
        "    return absa_model, polarity_model\n",
        "\n",
        "# training ABSA models\n",
        "# Using stratifiedKfold to try balancing classes during training.\n",
        "# This ensures that all classes are almost equally represented in the \n",
        "## training data thus eliminating bias towards one.\n",
        "\n",
        "def train_absa_model(train_data, target, model, task='polarity'):\n",
        "    \"\"\"Train absa model.\n",
        "    Args:\n",
        "        word_vectors: train vectors.\n",
        "        target: Variable to be predicted (label).\n",
        "        model: predictive model.\n",
        "    \"\"\"\n",
        "    print('===========================Training ABSA model=======================\\n')\n",
        "    stratified_kf = StratifiedKFold(8, shuffle=True, random_state=77)\n",
        "    labels = target\n",
        "    p_scores = []\n",
        "\n",
        "    for i, (tr, val) in enumerate(stratified_kf.split(train_data, labels), 1):\n",
        "        X_train, y_train = train_data[tr], np.take(labels, tr, axis=0)\n",
        "        X_val, y_val = train_data[val], np.take(labels, val, axis=0)\n",
        "        \n",
        "        model.fit(X_train, y_train)\n",
        "        val_preds = model.predict(X_val)\n",
        "        score = log_loss(y_val, val_preds)\n",
        "        p_scores.append(score)\n",
        "        print(f'Fold-{i} log_loss: {score}')\n",
        "    print(f'Mean_log_loss: {np.mean(p_scores)}')\n",
        "    print('\\n===============Saving trained model=============================\\n')\n",
        "    \n",
        "    save_dir = make_dir('saved_models')\n",
        "    # Save model\n",
        "    if task=='absa':\n",
        "        pickle.dump(model, open(save_dir+'/absa_model.pkl', 'wb'))\n",
        "    else:\n",
        "        pickle.dump(model, open(save_dir+'/polarity_model.pkl', 'wb'))\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "# Making predictions\n",
        "def predict_on_test(test_data, model, task='absa'):\n",
        "    \"\"\"Make predictions on test data set with\n",
        "    the model.\n",
        "    Args:\n",
        "        test_data: test vectors\n",
        "        model: trained model\n",
        "        type: polarity or absa predictions\n",
        "    \"\"\"\n",
        "    predictions = model.predict(test_data)\n",
        "    print(f'Predictions_shape: {predictions.shape}')\n",
        "    \n",
        "    #Test-data performance scores\n",
        "    test_scores = []\n",
        "    if task == 'polarity':\n",
        "        for i in range(len(polarity_encoder.classes_)):\n",
        "            loss =     log_loss(polarity_labels_test[polarity_encoder.classes_[i]], predictions[:,i]) \n",
        "            test_scores.append(loss)\n",
        "        print(f'Mean Test_loss_{task}: {np.mean(test_scores)}')\n",
        "    else:\n",
        "        #print(f'Aspect classes: {aspect_encoder.classes_}')\n",
        "        for i in range(len(aspect_encoder.classes_)):\n",
        "            loss =     log_loss(aspect_labels_test[aspect_encoder.classes_[i]], predictions[:,i]) \n",
        "            test_scores.append(loss)\n",
        "        print(f'Mean Test_loss_{task}: {np.mean(test_scores)}')\n",
        "\n",
        "def inverse_predict(data: [str], tokenizer, model2):\n",
        "    \"\"\"Reverse numerical predictions for each word in a list from numeric\n",
        "    value to text.\n",
        "    \"\"\"\n",
        "    import yake\n",
        "    extractor = yake.KeywordExtractor(lan='en',\n",
        "                                      n=1,\n",
        "                                      top=5\n",
        "                                      )\n",
        "    data = np.array(data)\n",
        "    text = data.flatten().tolist()\n",
        "    for i, c in enumerate(data):\n",
        "        word_to_vec = tokenizer.transform(c)\n",
        "        aspects = extractor.extract_keywords(text[i])\n",
        "        # bar plots\n",
        "        plt.bar(dict(list(aspects)).keys(), dict(list(aspects)).values())\n",
        "        # Create the wordcloud object\n",
        "        wordcloud = WordCloud(width=480, height=480, margin=0).generate(' '.join(list(dict(list(aspects)).keys())))\n",
        "        # Display the generated image:\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis(\"off\")\n",
        "        plt.margins(x=0, y=0)\n",
        "\n",
        "        plt.show()\n",
        "        plt.savefig(f'./wcloud_{i}.png')\n",
        "        top_aspect_term = list(aspects)[-1][0]\n",
        "        #predictions = aspect_encoder.inverse_transform(np.argmax(model1.predict(word_to_vec), 1))\n",
        "        polarity_pred = polarity_encoder.inverse_transform(np.argmax(model2.predict(word_to_vec),1))\n",
        "\n",
        "        print(f\"The Review: {str(c[0])}_\\n is expressing a {str(polarity_pred[0])} sentiment about {top_aspect_term}\\n\")\n",
        "\n",
        "def get_data():\n",
        "    \"\"\"Run absa models on unseen data\"\"\"\n",
        "    #import scraper\n",
        "    test_data, _ = get_twitter_data()\n",
        "    return test_data\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "data_shape: (2506, 3)\n",
            "\n",
            "sentiment tweet distribution: positive    1184\n",
            "negative     548\n",
            "neutral      368\n",
            "Name: polarity, dtype: int64\n",
            "\n",
            "\n",
            "Number of unique aspect_terms: 79\n",
            "\n",
            "\n",
            "========================Text Preprocessing=============================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqymlKLhcZ7e"
      },
      "source": [
        "test_data = get_data()\n",
        "\n",
        "aspect_model, sentiment_model = build_models()\n",
        "#absa_model = train_absa_model(train_vectors, y_aspect, aspect_model)\n",
        "\n",
        "polarity_model = train_absa_model(train_vectors, y_polarity, sentiment_model, task='polarity')\n",
        "print('==============Running sentiment predictions=================')\n",
        "predict_on_test(test_vectors, polarity_model, task='polarity')\n",
        "\n",
        "# Run model against scraped twitter data\n",
        "inverse_predict(test_data, count_vectorizer, polarity_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvD9gYP7ckO8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2krWAxZDLqC"
      },
      "source": [
        "### lstm.py\n",
        "import os\n",
        "import re\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, log_loss, mean_squared_error\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the train data\n",
        "train_data = pd.read_csv('./train.csv')\n",
        "# shuffle the dataframe\n",
        "train_data = train_data.sample(frac=1, random_state=77).reset_index(drop=True)\n",
        "\n",
        "# Text Cleaning\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;:$!]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_?&]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "single_chars = re.compile(r'\\s+[a-zA-Z]\\s+')\n",
        "\n",
        "def clean_text(text: str)-> str:\n",
        "    \"\"\"\n",
        "    Preprocesses text and returns a cleaned\n",
        "    piece of text with unwanted characters removed\n",
        "    \n",
        "    Args:\n",
        "       text: a string\n",
        "    Returns: \n",
        "        Preprocessed text\n",
        "    \"\"\"\n",
        "    text = str(text).lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwords from text\n",
        "    text = single_chars.sub('', text) #remove single-characters\n",
        "    return text\n",
        "\n",
        "def remove_URL(text: str)-> str:\n",
        "    \"\"\"\n",
        "    Removes URL patterns from text.\n",
        "    Args:\n",
        "        `text`: A string, word/sentence\n",
        "    Returns:\n",
        "        Text without url patterns.\n",
        "    \"\"\"\n",
        "    url = re.compile('https?://\\S+|www\\.\\S+')\n",
        "    text = url.sub('',text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# Splitting data into train and test sets\n",
        "train_df = train_data.iloc[:2900]\n",
        "test_df = train_data.iloc[len(train_df):]\n",
        "\n",
        "train_df['text'] = train_df['text'].apply(clean_text)\n",
        "train_df['text'] = train_df['text'].apply(remove_URL)\n",
        "\n",
        "test_df['text'] = test_df['text'].apply(clean_text)\n",
        "test_df['text'] = test_df['text'].apply(remove_URL)\n",
        "\n",
        "\n",
        "\n",
        "# Categorizing polarity and aspect_term labels (multi-label formulation)\n",
        "# Train\n",
        "train_df['polarity'] = train_df['polarity'].astype(str)\n",
        "\n",
        "#remove nan class\n",
        "train_df = train_df[train_df['aspect_term'] != 'nan']\n",
        "test_df = test_df[test_df['aspect_term'] != 'nan']\n",
        "\n",
        "\n",
        "polarity_encoder = LabelEncoder()\n",
        "train_df['polarity'] = polarity_encoder.fit_transform(train_df['polarity'])\n",
        "\n",
        "#Test-labels\n",
        "polarity_labels_test = pd.get_dummies(test_df['polarity'], dtype='int32')\n",
        "\n",
        "print('\\n========================Text Preprocessing=============================\\n')\n",
        "# text lengths\n",
        "token_length = [len(x.split(\" \")) for x in train_df['text']]\n",
        "print(f'Max_token length: {max(token_length)}\\n')\n",
        "\n",
        "\n",
        "# split data into features and labels for training\n",
        "X = train_df['text']\n",
        "y_polarity = train_df['polarity'] #polarity target\n",
        "\n",
        "# Text prepocessing\n",
        "train_corpus = X.tolist()\n",
        "test_corpus = test_df['text'].tolist()\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(train_corpus)\n",
        "vocab_size = len(tokenizer.word_counts)\n",
        "print(f'\\nTrain Vocabulary size: {vocab_size}\\n')\n",
        "\n",
        "# Sequence lengths (vocabulary size in a given sequence)\n",
        "# Computing the vocabulary size per percentile\n",
        "print('===========Analyzing the vocabulary size per percentile==============')\n",
        "seq_lengths = np.array([len(s.split()) for s in train_corpus])\n",
        "print(f'{[(p, np.percentile(seq_lengths, p)) for p in [75, 80, 90, 95, 99, 100]]}')\n",
        "\n",
        "max_seqlen = 64\n",
        "\n",
        "# Train encodings (words/sentences >> int) with padding\n",
        "# Padding ensures that sequences are of the same length\n",
        "train_encodings = tokenizer.texts_to_sequences(train_corpus)\n",
        "train_encodings = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    train_encodings, maxlen = max_seqlen)\n",
        "polarity_labels = np.array(y_polarity)\n",
        "\n",
        "# Creating a train dataset\n",
        "aspect_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_encodings, polarity_labels))\n",
        "\n",
        "# Test encodings with padding\n",
        "test_encodings = tokenizer.texts_to_sequences(test_corpus)\n",
        "test_encodings = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    test_encodings, maxlen= max_seqlen)\n",
        "test_labels = np.zeros_like(polarity_labels_test) # Predictions placeholder\n",
        "\n",
        "# Test dataset\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
        " (test_encodings, test_labels))\n",
        "\n",
        "def encode(text):\n",
        "    text_encodings = tokenizer.texts_to_sequences(text)\n",
        "    text_encodings = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    text_encodings, maxlen= max_seqlen)\n",
        "    dataset = (tf.data.Dataset.from_tensor_slices((text_encodings)))\n",
        "    return dataset\n",
        "\n",
        "# Creating train and test batches\n",
        "\n",
        "# Train-validation split and batch creation\n",
        "aspect_dataset = aspect_dataset.shuffle(2000)\n",
        "\n",
        "val_size = (len(train_corpus)) // 7\n",
        "val_dataset_absa = aspect_dataset.take(val_size)\n",
        "\n",
        "aspect_dataset = aspect_dataset.skip(val_size)\n",
        "\n",
        "batch_size = 8\n",
        "aspect_dataset = aspect_dataset.batch(batch_size).repeat()\n",
        "\n",
        "val_dataset_absa = val_dataset_absa.batch(batch_size)\n",
        "print(f'Validation_size: {val_size}')\n",
        "print(f'Train_size: {len(train_corpus)}')\n",
        "\n",
        "# test batch creation\n",
        "test_batched = test_dataset.batch(batch_size)\n",
        "\n",
        "\n",
        "# Building the model\n",
        "# \n",
        "# The model consist of:\n",
        "#      * An Embedding layer (to generate word embeddings)\n",
        "#      A Bidirectional LSTM layer\n",
        "#      1 hidden Dense layers with the `relu` activation function\n",
        "#      An output Dense layer\n",
        "\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    \"\"\"Computing the RMSE metric.\"\"\"\n",
        "    return K.sqrt(K.mean((K.square(y_pred - y_true))))\n",
        "\n",
        "\n",
        "\n",
        "embedding_dim=64\n",
        "\n",
        "# ABSA model\n",
        "absa_model = tf.keras.Sequential([\n",
        "    layers.Embedding(vocab_size+1, embedding_dim),\n",
        "    layers.Bidirectional(\n",
        "        layers.LSTM(max_seqlen, return_sequences=True)),\n",
        "    layers.GlobalAveragePooling1D(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "absa_model.build(input_shape=(batch_size, max_seqlen))\n",
        "\n",
        "absa_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=2e-3),\n",
        "              loss='mae' ,\n",
        "              metrics=[rmse])\n",
        "\n",
        "\n",
        "K.clear_session()\n",
        "history = absa_model.fit(aspect_dataset,\n",
        "                         epochs=8,\n",
        "                         steps_per_epoch=300,\n",
        "                         validation_data=val_dataset_absa,\n",
        "                         verbose=1,\n",
        "                         )\n",
        "\n",
        "mean_score = list(history.history['val_rmse'])\n",
        "print(mean_score)\n",
        "loss = np.round(np.mean(mean_score), 2)\n",
        "print(f'Train_RMSE: {loss}\\n')\n",
        "\n",
        "# Saving the model\n",
        "def make_dir(dir_name: str):\n",
        "    \"\"\"Creates a new directory in the current working directory.\"\"\"\n",
        "    save_dir = os.path.join('./', dir_name)\n",
        "    if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "    else:\n",
        "        print(f'{save_dir}: Already exists!') \n",
        "    return save_dir\n",
        "\n",
        "make_dir('./saved_models')\n",
        "absa_model.save('./saved_models/sentiment_model.h5')\n",
        "\n",
        "# Making predictions\n",
        "predictions = absa_model.predict(test_batched)\n",
        "\n",
        "print(f'Absa_predictions_shape: {predictions.shape}')\n",
        "\n",
        "test_data_absa = test_df[['text']].copy()\n",
        "\n",
        "# Test RMSE:\n",
        "def _rmse(y_true, y_pred):\n",
        "    \"\"\"Computing RMSE metric (without keras backend).\"\"\"\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "polarity_test_rmse = _rmse(predictions, polarity_labels_test.values)\n",
        "print(f'\\nTest_RMSE: {polarity_test_rmse}\\n')\n",
        "\n",
        "test_scores = []\n",
        "for i in range(len(polarity_encoder.classes_)):\n",
        "    loss = log_loss(polarity_labels_test[polarity_encoder.classes_[i]], predictions[:,i]) \n",
        "    test_scores.append(loss)\n",
        "print(f'Mean Test_loss_polarity: {np.mean(test_scores)}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC4exSLbDMQG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}